# 3D visualization with ParaView

## Connecting remotely to a ParaView HPC session

Many HPC systems are set up with ParaView installed, and allow you to make use of their superior computational power to load and render 3-D fields.
This can simply be done by running ParaView on the HPC and using a VNC to access the GUI on the remote system.
But ParaView also allows you to connect a local ParaView installation to a server running on the HPC.
This way, the remote system only focuses on rendering the data and does not need to generate the GUI and stream it via VNC.

The Cartesius [visualisation documentation](https://userinfo.surfsara.nl/systems/cartesius/paraview_client_server) by SURFsara provides an excellent walkthrough for setting up this ParaView connection.
On the HPC side, you need access to the GPU visualisation nodes, from which you can run the following commands
```
module load 2020
module load remotevis
gcn_paraview -w 1h -n 1 1
```
This will submit a job requesting 1 GPU nodes with a single process for 1 hour.
Once the job is ready, information on how to connect will appear.
On your *local* machine, now run a command such as
```
ssh -L 11111:gcn4:11111 username@vis.cartesius.surfsara.nl
```
The exact command you should run, specifying the host node and ports, will be shown by the output of `gcn_paraview` on the HPC.

ParaView must be installed on your machine before you can connect.
Specifically, you need the **_same version_** of ParaView as is used by `gcn_paraview` on the HPC.
For Cartesius, the software stack `2020` currently uses version `5.8`.

**Note**: the `2021` software stack that will be used on the new Snellius HPC does not currently have a `remotevis` module but does have version `5.9` of ParaView available, so we should expect the new `remotevis` (or equivalent) to use this version.

Finally, with the correct version of ParaView installed, you can connect to the remote server with the following steps:

1. *File* -> *Connect*
2. Optionally add the server with Add Server (host `localhost`, port `11111`, server type `Client / Server`, Startup Type `Manual`)
3. Select `cs://localhost:11111` in the Choose Server Configuration dialog
4. Click *Connect*

## Using the AFiDTools module to make the simulation output readable from ParaView

We can load the data into ParaView by using XDMF.
This data format uses XML to inform ParaView about the structure of the HDF5 files that we want to load, so we need to produce these XML files.
Python and Julia functions are provided to produce these XML files for the 2D slices stored in `outputdir/flowmov` and the 3D fields stored in `outputdir/fields`.
A (somewhat poorly maintained) overview of XDMF and the structure of these XML files can be found on the [XDMF website](https://www.xdmf.org/index.php/XDMF_Model_and_Format).

### 2D slices from `flowmov`
Although viewing the 2D slices can be easily done in [Jupyter notebooks](../jupyter), it is also possible to load the planes in ParaView.
This also has the benefit of plotting the various slices in 3-D space.
The Julia function `xmf_cuts()` produces an XML file for each variable in each plane slice.
These can be opened in ParaView using the XDMF Reader.

### 3D fields from the `fields` directory
The grids used by AFiD can be represented in three ways in XDMF through the `TopologyType` specified in the XML:

- 3DSMesh: This is the most generic form of a structured grid. It requires three 3D arrays specifying the coordinates $x$, $y$, and $z$ for each point of the arrays describing the flow field. These grids are not output by AFiD, but can be generated by the Python function `create_3D_grids` from the AFiDTools module. Volume rendering is *possible* in ParaView when this type of grid is used, but it is very slow.
- 3DRectMesh: This type describes a structured grid with perpendicular axes, as is used by AFiD. The grids from `outputdir/cordin_info.h5` can be used to describe the coordinates of the arrays. However, volume rendering is not possible in ParaView using this type of mesh.
- 3DCoRectMesh: This describes a grid with uniform grid spacing in each direction. This is the best type of grid for high-performance volume rendering. However, if grid stretching is used by the simulation, this will distort the visualisation of the fields. We aim to soon produce a small Fortran program using the interpolation routines of MuRPhFi to convert 3D fields to uniform grids for visualisation.